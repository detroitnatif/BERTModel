{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82386f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "import random\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import pathlib\n",
    "import sklearn\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import glob\n",
    "import functools\n",
    "\n",
    "TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75959b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "dataset_path = '/Users/tylerklimas/Desktop/BERTModel/dataset_processed'\n",
    "dataset_raw = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "labels = dataset_raw['train'].features['label'].names\n",
    "labels\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for idx, ele in enumerate(labels):\n",
    "    label2id[ele] = idx\n",
    "    id2label[idx] = ele\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8123b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = \"distilbert-base-uncased\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(base_model,\n",
    "                                                                        num_labels = len(labels),\n",
    "                                                                        label2id=label2id,\n",
    "                                                                        id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edeab9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand', 'item_id', 'item_name', 'main_image_id', 'node'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = set(dataset_raw['train'].column_names) - set(['text', 'label'])\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58533513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset_raw.map(tokenize_function, batched=True, remove_columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b059e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = tokenized_datasets['train'].num_rows\n",
    "subset = 20 # JUST FOR INFERENCE PURPOSE\n",
    "\n",
    "test_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(subset))\n",
    "test_dataset.set_format(type='torch')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "429372c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.96s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.05}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "utils.prediction_batch(model, test_dataset)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10e23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_int8 = torch.quantization.quantize_dynamic(\n",
    "                       model,\n",
    "                       {torch.nn.Linear},\n",
    "                       dtype = torch.qint8)\n",
    "# quantized_model_int8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cbae642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB):  267.942302\n",
      "Size (MB):  138.729314\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_size_model(model):\n",
    "    torch.save(model.state_dict(), 'temp.p')\n",
    "    print('Size (MB): ', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "    \n",
    "print_size_model(model)\n",
    "print_size_model(quantized_model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f347c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = \"distilbert-base-uncased\"\n",
    "script_tokenizer = transformers.AutoTokenizer.from_pretrained(base_model, torchscript=True)\n",
    "script_model = transformers.AutoModelForSequenceClassification.from_pretrained(base_model,\n",
    "                                                                        num_labels = len(labels),\n",
    "                                                                        label2id=label2id,\n",
    "                                                                        id2label=id2label,\n",
    "                                                                        torchscript=True)                                               \n",
    "                                                                               \n",
    "                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c711e025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:246: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "text = 'mens dress shoes'\n",
    "\n",
    "res = script_tokenizer.encode_plus(text, return_tensors='pt', padding='max_length', truncation=True)\n",
    "\n",
    "text_tokens = res['input_ids'].to(device)\n",
    "text_attentions = res['attention_mask'].to(device)\n",
    "\n",
    "dummy_input = [text_tokens, text_attentions]\n",
    "\n",
    "script_model = script_model.to(device)\n",
    "\n",
    "traced_model = torch.jit.trace(script_model, dummy_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1f20b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/tylerklimas/Desktop/BERTModel/TrainedModels/tokenizer_config.json',\n",
       " '/Users/tylerklimas/Desktop/BERTModel/TrainedModels/special_tokens_map.json',\n",
       " '/Users/tylerklimas/Desktop/BERTModel/TrainedModels/vocab.txt',\n",
       " '/Users/tylerklimas/Desktop/BERTModel/TrainedModels/added_tokens.json',\n",
       " '/Users/tylerklimas/Desktop/BERTModel/TrainedModels/tokenizer.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = '/Users/tylerklimas/Desktop/BERTModel/TrainedModels'\n",
    "model.save_pretrained('/Users/tylerklimas/Desktop/BERTModel/TrainedModels')\n",
    "tokenizer.save_pretrained('/Users/tylerklimas/Desktop/BERTModel/TrainedModels')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8da99b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_config = {\n",
    "    'model_name':\"pt-original\",\n",
    "    \"do_lower_case\": tokenizer.do_lower_case,\n",
    "    \"num_labels\":len(id2label),\n",
    "    'save_mode':\"original\",\n",
    "    'max_length': tokenizer.model_max_length,\n",
    "    'captum_explanation': True,\n",
    "    \"base_model\": 'distilbert-base-uncased',\n",
    "    'top_k': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34bd0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'{model_dir}/setup_config.json', 'w') as f:\n",
    "    json.dump(setup_config,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_config_trace = {**setup_config}\n",
    "setup_config_trace['model_name'] = 'pt-jit'\n",
    "setup_config_trace['capture_explanation'] = False\n",
    "setup_config_trace['save_model']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
